{"cells":[{"cell_type":"markdown","source":["This tutorial is an introduction to PySpark and the RDD API. Most of the examples are based on or isnpired by the following books:\n\n\n*   [Learning Spark, 2nd Edition](https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/)\n*   [Spark: The Definitive Guide](https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/)"],"metadata":{"id":"8rbh8z52xVP0","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9fff2c95-0ca7-45ca-b4ce-f465a8c3f46d","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# PySpark \nPySpark is an interface for Apache Spark that allows users to write Spark applications using python APIs. PySpark supports most of Spark’s features such as Spark SQL, DataFrame, Streaming, MLlib (Machine Learning) and Spark Core. For detailed information on these components and APIs, please refer to the [official PySpark Documentation](https://spark.apache.org/docs/latest/api/python/index.html)."],"metadata":{"id":"1OyVRQ2SxX1M","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c1026e5e-6b70-4acb-9861-7ca20fc73dd0","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## SparkSession\n\n[pyspark.sql.SparkSession](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.html) is the entry point to programming in Spark. It is mainly used to work with the higher-level APIs in Spark (Dataset and DataFrame) as we will see in the next tutorial. In this tutorial, we use it only to access SparkContext and work with the low-level API (The RDD API)."],"metadata":{"id":"_nohsgJ-0jJw","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2e5006de-58df-4968-a22d-e5be6cae7710","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Check Spark Session Information\nspark"],"metadata":{"id":"rPJvB-sJ1MRy","colab":{"base_uri":"https://localhost:8080/","height":219},"outputId":"2a2093af-15a2-4319-9b91-2ce03617756a","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4ac09f2c-a441-42cd-91ee-ca59abd1c2ae","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## SparkContext\n[pyspark.SparkContext](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.html#pyspark.SparkContext) is the entry point for low-level APIs in Spark."],"metadata":{"id":"VWpfTkUq-p95","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f6e112b2-3fbc-4dad-a163-7b98096cf150","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["sc = spark.sparkContext\nsc"],"metadata":{"id":"M1l3w7GjD3Hf","colab":{"base_uri":"https://localhost:8080/","height":196},"outputId":"12740c9c-73ed-4a7c-fcfc-896f4ed20d95","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7f408c42-a5f4-4f48-88c3-0f1e1fdc0dd4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Resilient Distributed Datasets (RDDs)\nRDD is the basic abstraction in Spark. It represents an immutable, partitioned collection of elements that can be operated on in parallel. While [Python's RDD API](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html#pyspark.RDD) provide the main methods for working with RDDs, [Scala's RDD API](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html) is the native detailed API for working with RDDs."],"metadata":{"id":"Qi3awsMvxxkM","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"035b35b1-3fb2-444f-8e1b-4ff4f0009070","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Parallelized Collections: Creating RDDs from Local Collections \nParallelized collections are created by calling SparkContext’s [`parallelize`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.parallelize.html#pyspark.SparkContext.parallelize) method on an existing iterable or collection in your driver program. The [`parallelize`] method turns a local collection into a parallel\ncollection. When creating this parallel collection, you can also explicitly state the number of\npartitions into which you would like to distribute it.\nThe elements of the collection are copied to form a distributed dataset that can be operated on in parallel."],"metadata":{"id":"67ISQhUd-vHa","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"08f07b94-b826-4f25-8347-b451e48028ef","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# parallelize a local collection \nrangeRDD = sc.parallelize(range(0, 30, 1))"],"metadata":{"id":"MbgPEW_LZUiQ","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5712d31c-824c-4ea7-845a-1f8b8c9e3cde","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#rangeRdd is just a logical object, nothing has been created yet \nrangeRDD"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i9eCsh_9MSf7","outputId":"dc5f3e2f-6321-46ba-8c3d-a81ae40f8b5a","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f48b8cfb-fecf-4201-9ef8-e6962068e3bc","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#you need an action to execute \nrangeRDD.glom().collect() \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4bZQLQcgNxvu","outputId":"df50be15-56fd-4c60-da58-335d58bd348c","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b969e836-3f4d-4728-aa20-e3925be3a98d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["[pyspark.RDD.glom](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.glom.html) returns an RDD created by coalescing all elements within each partition into a list. It is used here just to show the contents of each partition."],"metadata":{"id":"k2uLZzkd2rfa","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"410d9cd8-3d8a-484e-8870-ccb500db7100","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#try with a list of words\nwords = \"Singapore Data Science Consortium : Scalable Data Science with Apache Spark - Apache Spark Fundamentals\"\\\n.split(\" \")"],"metadata":{"id":"KUPlKCNH-zs2","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1fdca6fc-6778-4be2-8b0d-556d16acb721","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["wordsRDD = sc.parallelize(words)\nwordsRDD"],"metadata":{"id":"zmfJG37y_NXn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"da88cc29-76a3-40e1-8de1-2c28444b16ea","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e3a9c836-bf23-460d-9ab9-f859142fb363","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["wordsRDD.glom().collect() "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p46yiyYuBSlu","outputId":"2874f941-13c8-4a41-c5d4-1bafa51fbfa7","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2ecbc193-2ed1-4353-8196-c231fa7e5376","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Number of Partitions \n[getNumPartitions](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.getNumPartitions.html) returns the number of partitions in RDD."],"metadata":{"id":"jx--E8ircmUD","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c3a9b56b-5589-41fb-b2ad-525547aaf771","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["rangeRDD.getNumPartitions()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EgMwPvs0DRaB","outputId":"9f212c67-8473-4694-888b-f76cc429504f","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b9f99c68-5152-450a-9a4f-26ebb18fa837","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["wordsRDD.getNumPartitions()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DJ5-36UODaxg","outputId":"b9467767-9bf2-4b70-9ed2-2339ea25a20a","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1781e25a-0a6e-46bc-b684-5e4a445478e9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["One important parameter for parallel collections is the number of partitions to cut the dataset into. Normally, Spark tries to set the number of partitions automatically based on your cluster. However, you can also set it manually by passing it as a second parameter to `parallelize`."],"metadata":{"id":"RHeldihrEY7Y","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8331be06-be8d-4f15-9836-ff46265b6d8f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# parallelize a local collection with a specific number of partitions\nrangeRDD = sc.parallelize(range(0, 30, 1), 5)\nrangeRDD.getNumPartitions()"],"metadata":{"id":"20NvMjoyc3ei","colab":{"base_uri":"https://localhost:8080/"},"outputId":"530ee199-7fbf-4558-b3ea-d998cf53f136","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2d9c530e-a575-4d0e-a7dd-9d20b1f8231a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rangeRDD.glom().collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5gv2zjDWDpQ4","outputId":"2052a098-03b8-4c69-ef98-edddbbfc9c96","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8dc52c34-a72c-416d-87d1-eceb416eebd3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# try with the list of words\nwordsRDD = sc.parallelize(words,4)\nwordsRDD.getNumPartitions()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aZ6tsbt9CYGE","outputId":"53923c28-4d0d-4152-df47-08f4049a2f74","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1166aec8-494d-41eb-b020-e4849c451ee8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["wordsRDD.glom().collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Lbt8SLHDnuI","outputId":"21d14dc1-da44-4b77-e55b-be8b678a5851","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"beda41d0-ab3c-46ab-90c0-c2235443698e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Transformations\nCheck the list of common [transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations) in Spark."],"metadata":{"id":"TZAbwrEutPdL","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a70c5980-b33c-4c29-b009-c3c5fe7de5c0","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### distinct\n\n[pyspark.RDD.distinct](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.distinct.html) is used to return a new RDD that contains the distinct elements of the source RDD."],"metadata":{"id":"mZmb9Hu_IhAB","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5ec2f052-330f-41c8-9660-59eca886644c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["wordsRDD.distinct().count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GC5HmyC2IfXK","outputId":"28c86ba5-1e15-40cc-bc52-8cd6701e7425","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f5670ade-e9bf-4b3d-929b-dc4aeade2e6a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### filter\n[pyspark.RDD.filter](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.filter.html) returns a new RDD containing only the elements that satisfy a predicate."],"metadata":{"id":"f4PNoYvQIuWD","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"be82f7e1-6e48-44d8-8112-9af52d453d43","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["rangeRDD.filter(lambda x: x % 3 == 0).glom().collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Oy_aqhp1ghu","outputId":"f4d19091-979e-4242-e757-947d78b739e0","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"81bfa911-72b9-413a-a9f2-4c1a0bab5a71","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def startsWithS(word):\n  return word.startswith(\"S\")"],"metadata":{"id":"e8Y8Zm3tIv70","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bcc63b87-491f-4ab3-9c08-f08d2e5f6ab9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["wordsRDD.filter(lambda word: startsWithS(word)).glom().collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PXVkG3jUI4M9","outputId":"b340b4be-5b60-4bf7-aa58-5627f4dee3d7","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4bf67fb9-7f41-4f98-af24-3efd9d1cf4b9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### map\nIn this example, we’ll map the\ncurrent word to the word, its starting letter, and whether the word begins with “S.”"],"metadata":{"id":"NAM1GQ2EJTCI","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c26aea2a-11a4-4968-9e4f-b7cd79371346","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["wordsRDDmap = wordsRDD.map(lambda word: (word, word[0], word.startswith(\"S\")))"],"metadata":{"id":"Qt-1VqP9JYAA","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4070a8be-2854-4ec1-bebe-2f3baa3aa9c2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["wordsRDDmap.glom().collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UN5TZVzGJpSo","outputId":"19b79473-119d-4d0f-9785-dbeeb6b26ccd","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7479431e-aa27-4eab-ac14-94ccd438d99a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You can subsequently filter on this by selecting the relevant Boolean value:"],"metadata":{"id":"UQD62PciOESV","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"295d09e4-6ee5-46f2-90ef-b8aef6abbdeb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["wordsRDDmap.filter(lambda word: not word[2]).take(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"usjEkQrqODTN","outputId":"e03f276d-07ad-4390-e7a3-725ba19d36a1","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0e54de6a-fb9e-4afa-b10f-78108f77404c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### flatMap \n[pyspark.RDD.flatMap](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.flatMap.html) provides a simple extension of the map function we just looked at. Sometimes, each current item should return multiple items. flatMap returns a new RDD by first applying a function to all elements of the RDD, and then flattening the results.\nFor example, you might want to take your set of words and flatMap it into a set of characters."],"metadata":{"id":"9wtRW2b-VSbu","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"53d37d0c-5b52-46e0-a890-2d762ebd181b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["wordsRDD.flatMap(lambda word: list(word)).glom().take(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qonnpekN38Df","outputId":"327d2a37-d5cc-412d-d1a1-a6b0aabe302e","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a7ba2f62-e500-446f-a883-77c1b3fc439d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["wordsRDD.map(lambda word: list(word)).glom().take(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p733zWDYVHQX","outputId":"a0aa2511-074a-4c79-b8e2-e76a6a035582","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dcca896d-7ec3-4f9d-b354-a9da25f0d6be","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### mapPartitions\n[pyspark.RDD.mapPartitions](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.mapPartitions.html) is similar to map, but applies a function to each partition rather than each element in the partition."],"metadata":{"id":"qucLk7_-wk1e","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c7c0b4f2-5404-48fe-9233-e1cb2ba8a4c3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["rangeRDD.mapPartitions(lambda part: [sum(part)]).collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bAIKDGdNxebF","outputId":"6e736ac9-1981-437b-babe-4d4c6462cb8f","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e8810d15-e04b-4662-bb9a-f8b37268257a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### sample\n[pyspark.RDD.sample](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.sample.html) is used to sample a fraction fraction of the data."],"metadata":{"id":"NhZ0BOEQBrJl","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"000e0506-b456-4b08-8a8d-a4c336fbcfa5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["rangeRDD.sample(withReplacement = False,\n                fraction = 0.2,\n                seed = 33).count() "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jud6hE_YB-6A","outputId":"be3864e1-a190-4071-8b7d-9a172c097fec","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3774d325-f564-4ece-b5f2-a5916deda132","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### sortBy\nTo sort an RDD you can use the [pyspark.RDD.sortBy](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.sortBy.html) method. You specificy a function to extract a value from the elements in your RDD and then sort based on that. For instance, the following example sorts by word length from longest to shortest:"],"metadata":{"id":"rnTmTkOUWQdW","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a2b231f7-dcd9-4946-9f81-fba0c4dfc75b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["wordsRDD.sortBy(lambda word: len(word), ascending=False).collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lM-jbD8XWTDf","outputId":"be67b6fe-021c-40f2-a879-7bbcc18c9942","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"04169ac8-3c81-4656-9131-8401a1bb79ea","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### randomSplit\nWe can also randomly split an RDD into list of RDDs by using the [pyspark.RDD.randomSplit](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.randomSplit.html) method,\nwhich accepts an Array of weights and a random seed:"],"metadata":{"id":"Bbi8TDnEXO_f","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"153d02e4-7b4d-47bd-8e80-f3c66bb6cbf7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["fiftyFiftySplit = wordsRDD.randomSplit([0.3, 0.7])"],"metadata":{"id":"HCyUSRxfXSGv","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"971ddbeb-4a52-4adc-9617-44e82883811f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Actions\n\nCheck the list of common [actions](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions) in Spark."],"metadata":{"id":"kDLOxXgmzjBT","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"815178d9-6b8b-4c7a-b7b0-68163dda358d","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###reduce\nYou can use the [pyspark.RDD.reduce](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduce.html) method to specify a function to “reduce” an RDD of any kind of value\nto one value. For instance, given a set of numbers, you can reduce this to its sum by specifying a\nfunction that takes as input two values and reduces them into one."],"metadata":{"id":"EM5cQhgoFTpl","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"48de9614-e3c1-4339-8adc-27806347c179","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["rangeRDD.reduce(lambda a, b: a + b)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ac5K7VoFQe1","outputId":"da05fc04-5e0f-40f9-9143-967f3bdfaa5d","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1707ea88-1e2f-484a-9ece-5b033c8c6000","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You can also use this to get something like the longest word in our set of words. The key is just to define the correct function:"],"metadata":{"id":"TTfUVKkiinCS","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dcd9a719-3f8b-40c8-a3a3-d8394622af3e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def wordLengthReducer(leftWord, rightWord):\n  if len(leftWord) > len(rightWord):\n    return leftWord\n  else:\n    return rightWord\n\nwordsRDD.reduce(wordLengthReducer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"d4bHN2DliryH","outputId":"1f8fe461-e2a5-4fb0-921b-2c75626baf12","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"83eb70d2-9285-4ab3-965b-52220e475b95","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### count\n[pyspark.RDD.count](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.count.html) is a commonly used action to count the number of elements in the RDD."],"metadata":{"id":"jKrA-E-akXsM","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f3e10d70-fb89-417e-97c0-31e71f78dd3d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["wordsRDD.count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1kwV1bcDkng-","outputId":"502b5987-fa68-425d-83fa-4fc0edd9096e","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cda9bc33-942b-4896-a279-c89da32f0165","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rangeRDD.count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VeWG2bb4k__F","outputId":"a6575662-2c37-4efc-ccfa-14b22ddc1815","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d9f8ef54-84f5-4971-9344-73f69d5869bb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### first\nThe [pyspark.RDD.first](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.first.html) method returns the first value in the dataset:"],"metadata":{"id":"CrIQjtlJlXWW","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4c1e4222-b365-41c9-889e-7ec4d4dac8a4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["wordsRDD.first()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"CbXQFJzklmTW","outputId":"4f4f0b71-c2b0-4b70-9bee-f14538bc7783","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b5f958da-e084-4ec8-a3ba-2eef3c7f7415","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rangeRDD.first()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ImB7iLT4lW5_","outputId":"c10b7cf4-5c11-4d16-f485-a3a3cd97fe75","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c31f8480-fab3-4de3-b910-c9907dee854b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### min and max\n[pyspark.RDD.min](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.min.html) and [pyspark.RDD.max](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.max.html) return the minimum values and maximum values, respectively:"],"metadata":{"id":"lFkJgdl9lt9v","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5fb2773b-dd13-4c09-862c-02fb008b3a76","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["rangeRDD.min()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8WQpOPDVltKv","outputId":"7db3fc5c-d080-4ff9-87f4-ff6293a59d0d","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"209183ed-9f65-41df-9b6f-c8a8d3439566","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rangeRDD.max()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7krqozvRl8mH","outputId":"8f20d5f7-66a0-4f53-8e35-6e86f84f4a48","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d1fc7569-9227-4b45-8484-13bb2a272bd3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### take\n[pyspark.RDD.take](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.take.html) and its derivative methods take a number of values from your RDD."],"metadata":{"id":"NZnSG1Spn820","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ce2bfdce-bf50-4e98-8e9d-e19c00fe1c22","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["wordsRDD.take(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mMq5S5sSoEoE","outputId":"79d5362f-f35f-4470-df0c-498a1a26ead8","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b9074d9f-032e-4edb-97ed-c19095f0fb7c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You\ncan use [pyspark.RDD.takeSample](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.takeSample.html) to specify a fixed-size random sample from your RDD."],"metadata":{"id":"ysn5gxTKpemd","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ce75dbba-2d6d-4890-a6a3-941a31e8c94c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["wordsRDD.takeSample(withReplacement= True, \n                    num = 3,\n                    seed = 100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5wy-WojWorwE","outputId":"4c0a0a54-682b-4ba4-9ca5-6b6410dd2e8b","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b24838a6-0a33-4737-9e18-87131a085a46","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Working with Key-Value RDDs"],"metadata":{"id":"R1ADlawG-feW","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"121ebf47-8a73-4279-a62c-a5aa86b371a5","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["There are many methods on RDDs that require you to put your data in a key–value format. The most common operations on key-value RDDs are distributed “shuffle” operations, such as grouping or aggregating the elements by a key.\n\nThe easiest way is to make a key-value RDD is just to map over your current RDD to a basic key–value structure."],"metadata":{"id":"RTkNo0pkuq08","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4e94da6f-0d73-4778-8cfd-4e39efe2e09d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["wordsPairRDD = wordsRDD.map(lambda word: (word.lower(), len(word)))"],"metadata":{"id":"FK6Q87jwu6U1","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2dcede5f-e133-4385-8bb6-d7be7a7bf93f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["wordsPairRDD"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qymdvsu2Duuj","outputId":"8e15ec95-acf1-41c3-eadf-47ad5edf91ec","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"19c77bfa-903d-47bb-a7bd-c64e8bdbcb82","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["wordsPairRDD.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"frbf-c5jvE29","outputId":"2c1151ff-95ba-41af-cc07-551e0ea820e1","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c7d6858a-0767-4854-9d32-51f95483ee52","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### keyBy\nThe preceding example demonstrated a simple way to create a key for your RDD. However, you can also use the [pyspark.RDD.keyBy](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.keyBy.html) method to achieve the same result by specifying a function that creates the key from\nyour current value. In this case, you are keying by the first letter in the word. Spark then keeps\nthe record as the value for the keyed RDD:"],"metadata":{"id":"-vLpKMDlvRN1","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"50d9d205-bf80-4095-abdc-d8cfee805717","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["wordsPairRDD = wordsRDD.keyBy(lambda word: word.lower()[0])"],"metadata":{"id":"ls9Tk49qvoMV","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1f961e02-f840-4961-a67e-0f8c5fe89d92","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["wordsPairRDD.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kjUhVNzsvuH9","outputId":"02272b8f-e32c-4c40-eb04-035f2007faed","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d9e8454e-d471-4917-a07a-8c8f5a779beb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### mapValues\nIf you have a\ntuple, Spark will assume that the first element is the key, and the second is the value. When in\nthis format, you can explicitly choose to map-over the values without changing the keys using the [pyspark.RDD.mapValues](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.mapValues.html) method."],"metadata":{"id":"mk_ekcnqxPHe","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5309c6f6-d04b-4c79-8977-f91260688a8e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["wordsPairRDD.mapValues(lambda word: word.upper()).collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WM45kvUTxbGO","outputId":"0503a9ce-cd5e-40c9-f78c-1eb8def7542f","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4033229c-2c03-4f85-a5ea-ff588fa7695e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Extracting Keys and Values\nYou can extract the [pyspark.RDD.keys](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.keys.html) and [pyspark.RDD.values](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.values.html) of your key-value RDD:"],"metadata":{"id":"EO4kID1IyUO_","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"24df4f52-0839-46a6-b41a-bfa5a9290063","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["wordsPairRDD.keys().collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gBlWVKY3yZdP","outputId":"9e51f2c7-efcf-4ba2-a1aa-706a3645bef4","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b1ec5593-bdfa-4cac-9618-697843b4cc2e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["wordsPairRDD.values().collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"674bNNiSycwX","outputId":"e427835f-23f7-4af1-cd65-d4b434480cc1","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"97706871-f34d-406d-aba6-217c2758c9f7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### lookup\nOne interesting task you might want to do with an RDD is [pyspark.RDD.lookup](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.lookup.html) the values for a particular key."],"metadata":{"id":"E2SYynsHzFST","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f2cdd6b0-0cdb-49ca-965e-180df53253f4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["wordsPairRDD.lookup('s')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5EV5NVzVzbkU","outputId":"6bdefb70-8397-4702-e3f8-f49542226bf5","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eade8999-f0e1-4ef4-8212-ec2396af6627","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### countByKey\nWith [pyspark.RDD.countByKey](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.countByKey.html) you can count the number of elements for each key, collecting the results to a local dictionary."],"metadata":{"id":"KqZvDzRk4sL2","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fdbf0291-b125-4deb-b8d0-a43c2567d591","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["wordsPairRDD.countByKey()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j-clarPd5J9g","outputId":"d96595ba-a503-4f67-85d3-1e4b8f8dbdd5","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"97473eea-de7e-4187-bdb0-7568dcf64209","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### countByValue\n[pyspark.RDD.countByValue](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.countByValue.html) is used to return the count of each unique value in this RDD as a dictionary of (value, count) pairs."],"metadata":{"id":"HQ1kwpxe5fd6","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"082405d3-f8cd-4a04-9cec-0de9bcbf3f7b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["wordsRDD.countByValue()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qkwM53oJ53HB","outputId":"e8e70697-2bf0-4973-910d-ce7eb4411966","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bc073034-eecc-4cee-9591-48083d8d0fd2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### groupByKey\n[pyspark.RDD.groupByKey](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.groupByKey.html) is used to group the values for each key in the RDD into a single sequence."],"metadata":{"id":"hcrkLOyZI6Wm","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bc29f4fe-64e5-4648-bf0e-f1fb04483bb8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["wordsPairRDD.groupByKey().mapValues(list).collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xTuQsFuJJaMx","outputId":"270f3c49-1dd5-43ce-db57-b7dcdf519abd","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3402b637-cb25-4efa-8e4b-2a8978c40809","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["However, this is, for the majority of cases, the wrong way to approach the problem. The fundamental issue here is that each executor must hold all values for a given key in memory before applying the function to them. If you have massive key skew, some partitions might be completely overloaded with a ton of values for a given key, and you will get OutOfMemoryErrors. This obviously doesn’t cause an issue with our current dataset, but it can cause serious problems at scale. \n\nThere are use cases when groupByKey does make sense. If you have consistent value sizes for each key and know that they will fit in the memory of a given executor, you’re going to be just fine. It’s just good to know exactly what you’re getting yourself into when you do this."],"metadata":{"id":"YLxAfjAsLqqJ","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"db367843-a6b3-490b-975f-41a00c01021b","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["There are other funtions for aggregations such as\nreduceByKey, aggregateByKey, combineByKey, SortByKey,..."],"metadata":{"id":"RP7p8WO7MYXa","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3a95dda4-9416-4c41-b8ae-656e30586378","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## RDD Persistence\nYou can mark an RDD to be [persisted](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence) using the persist() or cache() methods on it. In addition, each persisted RDD can be stored using a different storage level set by passing a [pyspark.StorageLevel](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.StorageLevel.html#pyspark.StorageLevel) object to persist()."],"metadata":{"id":"4_V4j5EUBjtr","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"34e0fb99-4c19-474f-806b-124faed452a9","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Cache\nThe [pyspark.RDD.cache](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.cache.html) method is a shorthand for using the default storage level, which is StorageLevel.MEMORY_ONLY (store deserialized objects in memory)."],"metadata":{"id":"3BDWuBPbhI7-","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4a411666-bbdf-449c-a742-7c353b82ee08","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["wordsRDD.is_cached"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-wf5myq7IKmm","outputId":"4feafdff-aae8-4718-f205-d52a5e43fcf5","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3bede3c6-ba55-4017-bb5e-a33c1571c549","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["wordsRDD.getStorageLevel()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JDweMMPysIqi","outputId":"61d49ba8-2ff3-47c4-f77d-59f33a38265f","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c73e3b94-fe1f-484c-adea-1a9bb548b255","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["wordsRDD.is_cached"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0UPMAqhcsRAF","outputId":"4d089dde-7ebb-4afd-9e5c-ed1b5880f8d5","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ab917c50-35ba-4117-bdd1-3c31e557ae57","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Persist\nThe [pyspark.RDD.persist](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.persist.html) method is used to set the RDD's storage level to persist its values across operations after the first time it is computed."],"metadata":{"id":"OArMXmYHhQXd","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"44a8712d-4e55-4630-b6c1-524c607b03ba","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.storagelevel import StorageLevel\nrangeRDD.persist(storageLevel=StorageLevel.MEMORY_AND_DISK)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8IinPMtSrYtY","outputId":"7198fc11-1a9c-470d-cf86-aaef4e87b1ed","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c7d4f807-60fa-4009-bcf1-d20c735da591","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rangeRDD.getStorageLevel()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rdcHQ13_uZ9E","outputId":"befe3990-cef5-4265-bb1c-9883d06c02f2","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c187375c-b41d-4f3b-8625-5241dcaa259e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Text Files\nText file RDDs can be created using SparkContext’s [`textFile`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.textFile.html#pyspark.SparkContext.textFile) method. This method takes a URI for the file (either a local path on the machine, or a hdfs://, s3a://, etc URI) and reads it as a collection of lines. It returns one record per line in the file."],"metadata":{"id":"jxaA8pvThcZg","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"98ecb370-cdf8-45b0-aa45-0a3a27e39a64","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Databricks has its file system too\n# you can list the avaiable sample data:\ndisplay(dbutils.fs.ls('/databricks-datasets/'))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"083507cc-6457-4ac3-aae3-32a8a2e34b9d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["textRDD = sc.textFile(\"databricks-datasets/definitive-guide/README.md\", 8)"],"metadata":{"id":"sr5FSjvwhjZh","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8cf36186-ea67-45a5-be60-c0ff998450d7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["textRDD.take(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"553e6387-4156-47f5-83fd-becc62c29940","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#define lineLengths as the result of a map transformation. Again, lineLengths is not immediately computed, due to laziness.\nlineLengths = textRDD.map(lambda s: len(s))\n\n#run reduce, which is an action. At this point, the RDD is created, transformed, and the action is executed. \ntotalLength = lineLengths.reduce(lambda a, b: a + b)\n\n#this is the total number or words in the source file\ntotalLength"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JBNzK_htV5yo","outputId":"d5f0cf6c-4242-4310-e55e-a2f6b65a732a","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"06a2268f-fed1-4be8-a4c7-6d818ac5b90a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### CSV Files"],"metadata":{"id":"rk7PUl3Ge_wd","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f1a304d8-417d-443d-ae2a-2f84fb4717bd","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# define an RDD from csv file\n# the same textRDD method\nflightRDD =  sc.textFile(\"/databricks-datasets/definitive-guide/data/flight-data/csv/2015-summary.csv\", 10)"],"metadata":{"id":"z_kQC2-ofGou","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d3a64232-dc43-46c5-bef6-0b4dc5bc1b37","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["flightRDD.take(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4b65701c-2b5e-4021-884f-62c4a1c5269e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["flightRDD.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a043480b-c927-494b-aaab-aa298c67ee94","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark_part1","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2291072197012249}},"nbformat":4,"nbformat_minor":0}
